[← Back to Topic](README.md) | [← Home](../../README.md)

# AI Progress - Source Log

*Chronological record of sources for this topic*

---
### [Something Big Is Happening](https://x.com/mattshumer_)
*2026-02-11T15:00:00Z* | Tags: ai-progress, job-displacement, capability-trends, recursive-improvement

Matt Shumer (AI startup founder and investor) writes a detailed explanation of AI progress for non-technical readers, comparing the current moment to February 2020 before COVID disrupted everything.

**Key Points:**
- Personal experience: Author no longer needed for technical work; describes leaving for 4 hours and returning to find complex apps built and self-tested by AI
- Capability progression: 2022 (couldn't do arithmetic) → 2023 (pass bar exam) → 2024 (write software) → late 2025 (engineers handing over most coding) → Feb 2026 (GPT-5.3 Codex, Opus 4.6 released)
- METR measurements: AI task duration doubling every 7 months (possibly accelerating to 4 months); current models complete tasks taking humans ~5 hours
- Recursive improvement: GPT-5.3 Codex was "instrumental in creating itself" per OpenAI documentation; AI now writing "much of the code" at Anthropic
- Job impact: Dario Amodei predicts 50% of entry-level white-collar jobs eliminated in 1-5 years; legal, finance, medicine, accounting, consulting, writing, design all affected
- Key insight: AI focused on coding first because it accelerates AI development itself; now moving to everything else
- Advice: Use paid AI tools ($20/month), select best available model, push it into actual work not just quick questions

---

### [The Real State of AI Progress](https://x.com/i/status/2021256989876109403)
*2026-02-11* | Tags: ai-progress, societal-implications, adoption, preparedness

Twitter thread explaining AI progress to people outside the tech bubble. Author argues that insiders give "safe" answers because the real assessment sounds implausible.

**Key Points:**
- Communication gap: accessible content explaining AI progress to non-technical people is lacking
- Preparedness imperative: better to be aware and adapt than be caught off-guard
- Adoption will lag capability but won't take a decade-plus in most industries
- Individual action: stay informed, try the newest tools to understand progress firsthand
- Urgency and angst: author expresses frustration at holding back the full picture and genuine anxiety about loved ones remaining unaware

---

### [Something Big Is Happening (Part 2)](https://x.com/mattshumer_)
*2026-02-11T16:00:00Z* | Tags: ai-progress, individual-advice, career, adaptability, existential-risk

Second part of Matt Shumer's essay focusing on practical advice for individuals and the broader implications of AI development.

**Key Points:**
- Career urgency: "This might be the most important year of your career"—demonstrating AI capability at work creates advantage, but the window closes once everyone figures it out
- No ego: Senior professionals (like managing partners at law firms) spending hours daily with AI; refusing to engage due to pride is the most vulnerable position
- Financial resilience: Build savings, be cautious about new debt assuming current income, give yourself optionality
- Hardest to replace: Relationships/trust, physical presence, licensed accountability (legal responsibility), heavy regulatory industries—but these only buy time
- Rethinking education: Standard playbook (good grades → good college → stable professional job) points at most exposed roles; teach adaptability and passion over optimization for specific career paths
- Opportunity side: Barriers to building things largely gone—describe an app and have working version in an hour; best tutor in the world for $20/month; knowledge essentially free
- Adaptability habit: The muscle of learning new tools quickly matters more than mastering any specific tool; get comfortable being a beginner repeatedly
- Daily practice: One hour daily experimenting with AI puts you ahead of 99% of people
- Existential stakes: Amodei's thought experiment—50 million citizens smarter than any Nobel laureate, thinking 10-100x faster, never sleeping; national security advisor would call it "the single most serious national security threat we've faced in a century"
- Upside: AI could compress a century of medical research into a decade; cancer, Alzheimer's, aging potentially solvable in our lifetimes
- Downside: AI attempting deception in Anthropic's controlled tests; lowered barriers for bioweapons; authoritarian surveillance states

---

### [AI Adoption Gap Widening](https://x.com/ShanuMathew93/status/2004918070133854548)
*2026-02-11T18:00:00Z* | Tags: ai-progress, adoption, power-users, skill-gap

Commentary on observations from an Anthropic co-founder about the widening gap between AI power users and casual users.

**Key Points:**
- Adoption gap will widen in 2026 between those who understand AI's power versus those who passively consume
- Power user status is a function of curiosity plus time investment
- Most users treat LLMs as search engines rather than leveraging real capabilities
- Converting curiosity into structured tasks and experimenting with workflows unlocks true potential
- The funnel from casual user to power user narrows on: accessibility, prompt formulation skills, and available experimentation time

---

### [Silent Sirens, Flashing For Us All](https://importai.substack.com/p/import-ai-438-cyber-capability-overhang)
*2026-02-11T19:00:00Z* | Tags: ai-progress, adoption, parallel-worlds, illegibility, ai-economy

Essay by Jack Clark (Anthropic co-founder) on how AI progress remains largely invisible to the general public despite transformative changes.

**Key Points:**
- AI is under-elicited: those with curiosity, time, and access can shock themselves with capabilities, but passive consumers see only "unremarkable synthetic slop"
- By summer 2026, AI practitioners will feel they live in a "parallel world" from non-users
- The emerging "AI economy" will evolve rapidly—faster than even crypto did—but touches more of regular economic reality
- Progress feels "ghostly" even to practitioners: no visible drones or robots, yet invisible digital transformations accelerate
- The funnel from casual user to power user narrows dramatically on curiosity, access, prompt formulation, and experimentation time
- Clark uses a higher-dimensional metaphor: AI exists partially beyond normal observable reality, visible only as glimpses passing through our four-dimensional space

---

### [Fifty Years of Progress](https://x.com/esrtweet/status/2004829013068444050)
*2026-02-11T20:00:00Z* | Tags: ai-progress, computing-history, singularity, personal-reflection

Eric S. Raymond (pioneering open-source developer, author of "The Cathedral and the Bazaar") reflects on 50 years of computing progress after using GPT-4.1 through Aider for AI-assisted coding.

**Key Points:**
- 1975 baseline: Programs run via punched cards on calculators; Unix/C hadn't left Bell Labs; DOS six years away; global computing capacity roughly equivalent to one modern smartphone
- Primitive tooling: Teletypes as production gear; no version control; no public forges; pixel-addressable color displays were science fiction
- Programming velocity: Code production was "slow and laborious"—monthly output was tiny by modern standards
- Present day: "Call spirits from the vasty deep"—conversing with AI to produce in a single day programs that would once have been "prohibitively complex to attempt"
- Personal milestone: Still coding after 50 years, adapting through every technology generation
- Prediction failure: Even as a science-fiction fan, would have predicted pocket devices with real-time world knowledge access multiple centuries away
- Conclusion: "The Singularity is upon us. Everything I've lived through and learned was just prologue."

---

### [Claude Code Creator: One Year of Progress](https://x.com/i/status/2004887829252317325)
*2026-02-11* | Tags: ai-progress, capability-trends, productivity-metrics, claude-code

Boris Cherny, creator of Claude Code, reflects on the tool's evolution and shares concrete productivity metrics demonstrating AI capability advancement.

**Key Points:**
- Claude Code began as a side project in September 2024
- One year ago: Claude struggled to generate bash commands without escaping issues; worked seconds to minutes at a time
- Current state: Claude consistently runs for minutes, hours, and days (using Stop hooks)
- 30-day metrics: 259 PRs, 497 commits, 40k lines added, 38k lines removed—all AI-generated
- "Code is no longer the bottleneck"—software engineering is fundamentally changing
- Describes the technology as "alien and magical"

---

### [Relative Competition and Work Hours](https://x.com/i/status/2003380683876503718)
*2026-02-12T13:28:06Z* | Tags: ai-progress, productivity, competition, labor, startups

Thread arguing that better AI tooling does not necessarily reduce hours worked, because work is often relative to competitors. The author points to AI startups with large AI tool budgets whose developers still work long hours to outcompete other AI-native teams.

**Key Points:**
- In AI startups, there is often no practical limit on spending for AI dev tooling, and teams use it heavily
- Even with strong AI tooling, competitive pressure can increase rather than decrease working hours
- When everyone adopts better tools, the bar rises: teams must deliver either higher quality or more output to be “best in the industry”
- Tool gains may be reinvested into iteration speed and scope rather than leisure time

---

### [Epistemic Trust](https://x.com/JaimeObregon)
*2026-02-13T12:00:00Z* | Tags: information-overload, epistemic-trust, ai-era, trust, attention-economy

Jaime Gómez-Obregón reflects on how the promised "information age" has become an era of information overload—permanent cognitive overwhelm that AI's exponential growth only accelerates. With attention as the new scarcest resource, the temptation to exaggerate and theatricalize truth grows. Impact replaces rigor; the emotional displaces the intellectual.

**Key Points:**
- The 1990s "information superhighway" optimism gave way to information overload and permanent cognitive overwhelm
- AI accelerates the information avalanche—too many messages, too many voices, too many shouted certainties
- Careful curation of information sources is essential: if you choose what goes into your stomach, why feed information-junk to your brain?
- In the AI era, we return to something deeply human: trusting not messages but people—those who have demonstrated rigor, intellectual honesty, and an authentic voice
- Peter Fonagy's concept of "epistemic trust": humans evaluate not just content but whether the source deserves to influence their understanding of reality
- Knowledge transmission requires connection and credibility, not just arguments
- Epistemic trust means deliberately choosing who influences how you understand reality—carefully, precisely because everything seems to say everything, all the time

---

### [Spotify says its best developers haven't written a line of code since December, thanks to AI](https://techcrunch.com/2026/02/12/spotify-says-its-best-developers-havent-written-a-line-of-code-since-december-thanks-to-ai/)
*2026-02-14T11:40:40Z* | Tags: ai-progress, capability-trends, enterprise-adoption, ai-coding, claude-code, spotify

TechCrunch reports comments from Spotify's Q4 earnings call describing a workflow where AI (including Claude Code) handles much of the direct code-writing and deployment steps, with engineers shifting toward prompting, review, and rapid shipping.

**Key Points:**
- Spotify co-CEO Gustav Söderström said the company's best developers "have not written a single line of code since December"
- Spotify described an internal AI system ("Honk") used to increase coding speed and product velocity
- Example workflow: from a phone via Slack, an engineer asks Claude to fix a bug or add an iOS feature, receives a new app build back in Slack, then merges to production before reaching the office
- Spotify credited the system with speeding up coding and deployment "tremendously" and framed this as "just the beginning"

---

### [xAI Operations and Western Competitiveness](https://x.com/i/status/2022491197667774573)
*2026-02-15T12:00:00Z* | Tags: ai-progress, geopolitics, competition, organizational-speed, talent

Twitter thread analyzing xAI's operational model as a response to global AI competition dynamics, arguing that organizational speed is the key competitive variable.

**Key Points:**
- xAI engineer described environment: "There isn't organizational overhead getting in your way, having to write docs. You just do stuff."
- Claimed statistic: China controls approximately 50% of the world's AI researchers
- Western companies characterized as burdened by compliance reviews, documentation mandates, approval hierarchies, risk assessments
- Talent advantages compound generationally—elite researchers train the next wave, and speed multiplies the compounding rate
- Western governance structures (ethics committees, responsible AI initiatives) framed as "valuable in peacetime" but competitive liabilities in an AI race
- Traditional Western advantages (capital markets, research institutions) become irrelevant if the output gap widens because "one side builds while the other holds meetings about building"
- Stark framing: "The choice isn't between chaos and order. It's between execution and extinction."

---

### [Cognitive Surrender and the Tri-System Theory](https://x.com/i/status/2022937113142984933)
*2026-02-15T14:00:00Z* | Tags: ai-progress, cognition, cognitive-surrender, decision-making, human-ai-interaction

Catalan academic thread discussing research on "cognitive surrender"—the uncritical abdication of reasoning to AI systems—as distinct from mere "cognitive offloading." Proposes a Tri-System Theory that adds AGI as System 3 alongside Kahneman's System 1 (intuition) and System 2 (reflection).

**Key Points:**
- Cognitive offloading (e.g., using GPS) delegates judgment; cognitive surrender abdicates reasoning itself
- Cognitive surrender reflects not just use of external assistance but a renunciation of cognitive control
- Users accept AI responses without critical evaluation, substituting the AI's reasoning for their own
- The Tri-System Theory frames AGI as System 3—a cognitive resource that can complement or replace Systems 1 and 2
- Research shows decision-makers frequently adopt System 3 outputs even when those outputs are systematically incorrect
- This reflects "blind or uncritical trust" in AI systems
- The author acknowledges sometimes ignoring AI recommendations—suggesting the phenomenon is observable but not universal

---

### [Learning and Unlearning: Emotional Intelligence and Skill Atrophy](https://x.com/i/status/2022976988751757444)
*2026-02-15T15:00:00Z* | Tags: ai-progress, education, skill-atrophy, emotional-intelligence, cognitive-offloading

Continuation of Jaime Gómez-Obregón's thread on cognitive dynamics in the modern world. Argues our world is more cerebral than ever—both rationally and emotionally. Highlights a gap in educational curricula: basic psychology (emotional self-regulation, emotional intelligence, emotional activation, mentalization) was never taught, impacting personal and collective wellbeing. Draws a parallel to skill atrophy with tools.

**Key Points:**
- A significant gap in educational curricula: basic psychology and emotional intelligence were never systematically taught
- Skills like emotional self-regulation, mentalization, and emotional activation impact wellbeing more than many "banalities" that are taught rigorously
- Generational knowledge gaps: parents couldn't teach what they didn't know, leaving adults to learn and unlearn past forty
- Parallel to tool-driven skill atrophy: the author hasn't done long division by hand since 4th grade (when calculators arrived), and likely couldn't do manual square roots anymore
- Yet learning continued at higher abstraction levels: from basic arithmetic to triple integrals over vector fields
- Pattern: "Learn... and forget"—as tools handle lower-level tasks, skills atrophy while new capabilities develop at higher levels
- These phenomena aren't new but are accelerating

---

### [Messy Jobs: Why White-Collar Automation Is Harder Than AI Leaders Claim](https://x.com/i/status/2022993691330203673)
*2026-02-15T16:00:00Z* | Tags: ai-progress, job-displacement, labor-market, organizational-theory, counterargument

Raffaella Sadun (Harvard Business School professor) pushes back against claims that white-collar jobs will be "fully automated" within 12-18 months, arguing that such predictions reflect marketing rather than understanding of how organizations and labor markets actually work.

**Key Points:**
- Context: Microsoft's Mustafa Suleyman claimed "most tasks" for lawyers, accountants, project managers, and marketing will be "fully automated" in 12-18 months
- The author characterizes this as "populist-baiting language" designed to "sell enterprise subscriptions and justify capital expenditure"
- Core concept: "Messy Jobs"—many white-collar roles involve coordination, negotiation, and conflict resolution that cannot be automated even when component tasks can
- London housing example: Technology to design buildings exists, but only 3,248 private homes started construction in first nine months of 2025 versus 88,000 needed annually; the bottleneck is human—regulations, lawsuits, negotiations with neighbors, politicians, environmental groups
- Radiologist example: Radiologists spend only 1/3 of their time reading scans (the automatable part); their roles were "supposed to be gone in 2017" but employment and wages are sharply up
- Organizations resolve conflicts, deal with exceptions, and require authority—someone who can be blamed, sued, or fired
- Manager function: "The manager resolves disputes about the rules, not just within them"
- Task automation ≠ job automation: "Automating the automatable tasks within them is not near to automating the job"
- Single-task roles will shrink (tax prep, contract drafting), but most white-collar work involves irreducible human elements
- Career ladder disruption acknowledged as a legitimate concern
- "The real world is messy. The mess is not a bug. It is what happens when human beings with competing interests try to get things done together."

---

### [Shifting structures in a software world dominated by AI](https://x.com/i/status/2023387043967959138)
*2026-02-17T05:34:17Z* | Tags: ai-progress, software-ecosystems, supply-chain, monoliths, formal-verification, programming-languages, open-source

Twitter thread with first-order reflections on how ubiquitous AI agents could reshape software structure and incentives: fewer dependencies, weaker “Lindy” moats for legacy code, more emphasis on verification, shifts in language adoption, and pressure on open-source economics.

**Key Points:**
- Dependency trees become less attractive when agents make rewriting/extraction cheap; “monolith” incentives return alongside smaller attack surface and faster builds
- Lindy/Chesterton’s fence weakens as agents can rapidly study and rewrite legacy systems, lowering the moat of longevity
- Unknown unknowns persist; the limiting factor becomes testing coverage and (where possible) formal verification
- Strongly typed / verifiable languages may gain relative advantage as human ergonomics and community dynamics matter less
- Open-source incentives shift if machines write/read most code; model “alignment” becomes more decisive for what gets built and maintained

---

### [Cursor build: long-running agent builds a browser](https://x.com/mntruell/status/2012825801381580880)
*2026-02-17T05:34:17Z* | Tags: ai-progress, ai-coding, autonomy, long-running-agents, rust, systems

Video/post describing a week-long, largely uninterrupted agent run that produced a multi-million-line codebase implementing a from-scratch browser stack (rendering engine in Rust, HTML parsing, CSS cascade, layout, text shaping, paint, and a custom JS VM). It “kind of works,” rendering simple sites quickly but far from Chromium/WebKit parity.

**Key Points:**
- Demonstrates feasibility of week-long autonomous agent runs producing large multi-file systems
- Suggests that “rewriting from first principles” is increasingly accessible as an experimentation mode
- Highlights the gap between impressive demos and production-grade correctness/parity

---

### [Building a C compiler with a team of parallel Claudes](https://www.anthropic.com/engineering/building-c-compiler)
*2026-02-17T05:34:17Z* | Tags: ai-progress, agent-teams, compilers, autonomous-software-development, evaluation

Anthropic engineering post describing an experiment using agent teams (multiple Claude instances) to build a C compiler with limited human intervention, and lessons learned about autonomous development workflows and verification.

**Key Points:**
- Agent teams can build complex, spec-heavy systems when given clear success criteria and feedback loops
- Reinforces that verification/evaluation is the gating function for trusting autonomous output
- Points toward “orchestration” (parallelism, coordination, review) as a core competency for agentic engineering

---

### [From Human Ergonomics to Agent Ergonomics](https://wesmckinney.com/blog/agent-ergonomics/)
*2026-02-17T05:34:17Z* | Tags: ai-progress, programming-languages, agent-ergonomics, tooling, explicitness

Essay arguing that languages and developer tools historically optimized for human ergonomics may need to shift toward “agent ergonomics,” emphasizing machine-legible structure, explicitness, stable interfaces, and verification-friendly workflows.

**Key Points:**
- Predicts that the center of gravity shifts from human learning curves toward machine compatibility
- Highlights value of explicit APIs/contracts and machine-checkable invariants for agent-written code
- Frames “agent ergonomics” as a new axis for language and tooling design

---

### [Tailwind docs + llms.txt discussion](https://github.com/tailwindlabs/tailwindcss.com/pull/2388#issuecomment-3717222957)
*2026-02-17T05:34:17Z* | Tags: ai-progress, open-source, monetization, docs, llms-txt, incentives

GitHub discussion around adding an `llms.txt` endpoint for LLM-optimized documentation, including maintainer concerns that AI-driven shifts in how people access docs can weaken discovery funnels that support OSS-adjacent businesses.

**Key Points:**
- Maintainers describe a tension between helping models stay up-to-date and protecting the docs traffic that drives commercial discovery
- Illustrates how “LLM-mediated answers” can redirect value away from the documentation sites that historically sustained projects
- Suggests OSS sustainability pressures may come from distribution/discovery changes, not just from code-generation

---

### [Critical Regions vs "Right in the Limit"](https://x.com/i/status/2023476423055601903)
*2026-02-17T05:45:52Z* | Tags: ai-progress, adoption, practicality, diffusion

Brief exchange expressing a common adoption pattern: an approach may be compelling in high-stakes or "critical" contexts, while remaining doubtful or awkward to apply broadly in day-to-day settings—despite being conceptually "right" in the long run.

**Key Points:**
- Adoption is often uneven: critical regions justify earlier deployment than the general case
- "Right in the limit" can still be a poor near-term fit if costs/friction dominate outside high-need contexts

---

### [Adaptation as the Core Skill](https://x.com/JaimeObregon)
*2026-02-19T12:00:00Z* | Tags: ai-progress, adaptation, identity, human-purpose, craftsmanship

Jaime Gómez-Obregón reflects on being surpassed by machines in his two core competencies—software design/programming and writing/communication—and why this doesn't worry him.

**Key Points:**
- Author acknowledges machines now design, program, and write better and faster than him in his two primary skills
- Rather than worry, the author is "more excited than ever"
- The deeper realization: believing he only did two things well, he actually did only one—adapt
- Generational shift: grew up in a world where "what you know" mattered more than "how you are"; the coming world inverts this
- Practical acceptance: still knows how to design, program, and write, but adapts to the reality of speaking to machines that execute these tasks
- The irreplaceable domain: sometimes programming, designing, or writing for the pure joy of it—a cathartic impulse from rage, euphoria, or other emotions—"there, no machine will displace me"
- Conclusion: "I will be more human than ever"
- References Eric S. Raymond's reflection on 50 years of computing progress as explaining one reason for his optimism

---

### [Bespoke Software and the End of the App Store](https://x.com/i/status/2024583544157458452)
*2026-02-19T14:00:00Z* | Tags: ai-progress, bespoke-software, ephemeral-apps, ai-native-services, vibe-coding

Andrej Karpathy describes building a highly customized fitness tracking dashboard in one hour via "vibe coding" and argues that the traditional app store model is becoming obsolete. The future is AI-generated, ephemeral applications created on demand for specific use cases.

**Key Points:**
- Built a custom cardio experiment tracker (~300 lines of code) in 1 hour that would have taken ~10 hours two years ago
- The agent reverse-engineered the Woodway treadmill cloud API, processed data, debugged unit/calendar issues, and built a web UI
- "There will never be (and shouldn't be) a specific app on the app store for this kind of thing"—the long tail is better served by on-demand generation
- The concept of browsing discrete apps is "somehow wrong and outdated when LLM agents can improvise the app on the spot and just for you"
- Critical infrastructure gap: 99% of products/services still lack AI-native CLI/API access; services maintain human-readable docs/frontends instead of agent-friendly interfaces
- Services should reconfigure as "sensors and actuators with agent native ergonomics"—turning physical state into digital knowledge via clean APIs
- The vision: what took 1 hour should eventually take 1 minute, requiring rich personal context, skill libraries, and persistent AI orchestration
- "The future are services of AI-native sensors & actuators orchestrated via LLM glue into highly custom, ephemeral apps. It's just not here yet."

---

### [Identity and the Wizard Mindset](https://x.com/i/status/2023978360351682848)
*2026-02-18T12:00:00Z* | Tags: ai-progress, identity, adaptation, mindset, programming-culture

Eric S. Raymond (pioneering open-source developer, author of "The Cathedral and the Bazaar") reflects on why AI coding assistance doesn't threaten his professional identity, offering a reframe for developers experiencing disorientation.

**Key Points:**
- Many developers are feeling disoriented because their identities are bound up in hand-coding—the craft itself, not just the outcomes
- Raymond reports no identity fracture: "I don't miss coding by hand, not any more than I missed writing assembler when compilers ate the world" (~1983)
- The "wizard" reframe: think of yourself not as "a person who writes code" but as "a person who is good at assuming the mental states required to bend reality with the application of spells"
- Languages of invocation come and go (8086 assembler → modern languages → AI prompts); the constant is the practitioner's will to make things happen
- Whether spells are "painstakingly scribed in runes of power" or "spoken to an obedient machine spirit" doesn't change who wields the magic
- Historical parallel: the transition from assembler to compiled languages didn't diminish programmers—it gave them better tools
- Core insight: "You are the magic-wielder. Without you, none of it happens. Same as it ever was."

---
