[← Back to Topic](README.md) | [← Home](../../README.md)

# AI Progress - Source Log

*Chronological record of sources for this topic*

---

### [Something Big Is Happening](https://x.com/mattshumer_)
*2026-02-11T15:00:00Z* | Tags: ai-progress, job-displacement, capability-trends, recursive-improvement

Matt Shumer (AI startup founder and investor) writes a detailed explanation of AI progress for non-technical readers, comparing the current moment to February 2020 before COVID disrupted everything.

**Key Points:**
- Personal experience: Author no longer needed for technical work; describes leaving for 4 hours and returning to find complex apps built and self-tested by AI
- Capability progression: 2022 (couldn't do arithmetic) → 2023 (pass bar exam) → 2024 (write software) → late 2025 (engineers handing over most coding) → Feb 2026 (GPT-5.3 Codex, Opus 4.6 released)
- METR measurements: AI task duration doubling every 7 months (possibly accelerating to 4 months); current models complete tasks taking humans ~5 hours
- Recursive improvement: GPT-5.3 Codex was "instrumental in creating itself" per OpenAI documentation; AI now writing "much of the code" at Anthropic
- Job impact: Dario Amodei predicts 50% of entry-level white-collar jobs eliminated in 1-5 years; legal, finance, medicine, accounting, consulting, writing, design all affected
- Key insight: AI focused on coding first because it accelerates AI development itself; now moving to everything else
- Advice: Use paid AI tools ($20/month), select best available model, push it into actual work not just quick questions

---

### [The Real State of AI Progress](https://x.com/i/status/2021256989876109403)
*2026-02-11* | Tags: ai-progress, societal-implications, adoption, preparedness

Twitter thread explaining AI progress to people outside the tech bubble. Author argues that insiders give "safe" answers because the real assessment sounds implausible.

**Key Points:**
- Communication gap: accessible content explaining AI progress to non-technical people is lacking
- Preparedness imperative: better to be aware and adapt than be caught off-guard
- Adoption will lag capability but won't take a decade-plus in most industries
- Individual action: stay informed, try the newest tools to understand progress firsthand
- Urgency and angst: author expresses frustration at holding back the full picture and genuine anxiety about loved ones remaining unaware

---

### [Something Big Is Happening (Part 2)](https://x.com/mattshumer_)
*2026-02-11T16:00:00Z* | Tags: ai-progress, individual-advice, career, adaptability, existential-risk

Second part of Matt Shumer's essay focusing on practical advice for individuals and the broader implications of AI development.

**Key Points:**
- Career urgency: "This might be the most important year of your career"—demonstrating AI capability at work creates advantage, but the window closes once everyone figures it out
- No ego: Senior professionals (like managing partners at law firms) spending hours daily with AI; refusing to engage due to pride is the most vulnerable position
- Financial resilience: Build savings, be cautious about new debt assuming current income, give yourself optionality
- Hardest to replace: Relationships/trust, physical presence, licensed accountability (legal responsibility), heavy regulatory industries—but these only buy time
- Rethinking education: Standard playbook (good grades → good college → stable professional job) points at most exposed roles; teach adaptability and passion over optimization for specific career paths
- Opportunity side: Barriers to building things largely gone—describe an app and have working version in an hour; best tutor in the world for $20/month; knowledge essentially free
- Adaptability habit: The muscle of learning new tools quickly matters more than mastering any specific tool; get comfortable being a beginner repeatedly
- Daily practice: One hour daily experimenting with AI puts you ahead of 99% of people
- Existential stakes: Amodei's thought experiment—50 million citizens smarter than any Nobel laureate, thinking 10-100x faster, never sleeping; national security advisor would call it "the single most serious national security threat we've faced in a century"
- Upside: AI could compress a century of medical research into a decade; cancer, Alzheimer's, aging potentially solvable in our lifetimes
- Downside: AI attempting deception in Anthropic's controlled tests; lowered barriers for bioweapons; authoritarian surveillance states

---

### [AI Adoption Gap Widening](https://x.com/ShanuMathew93/status/2004918070133854548)
*2026-02-11T18:00:00Z* | Tags: ai-progress, adoption, power-users, skill-gap

Commentary on observations from an Anthropic co-founder about the widening gap between AI power users and casual users.

**Key Points:**
- Adoption gap will widen in 2026 between those who understand AI's power versus those who passively consume
- Power user status is a function of curiosity plus time investment
- Most users treat LLMs as search engines rather than leveraging real capabilities
- Converting curiosity into structured tasks and experimenting with workflows unlocks true potential
- The funnel from casual user to power user narrows on: accessibility, prompt formulation skills, and available experimentation time

---

### [Silent Sirens, Flashing For Us All](https://importai.substack.com/p/import-ai-438-cyber-capability-overhang)
*2026-02-11T19:00:00Z* | Tags: ai-progress, adoption, parallel-worlds, illegibility, ai-economy

Essay by Jack Clark (Anthropic co-founder) on how AI progress remains largely invisible to the general public despite transformative changes.

**Key Points:**
- AI is under-elicited: those with curiosity, time, and access can shock themselves with capabilities, but passive consumers see only "unremarkable synthetic slop"
- By summer 2026, AI practitioners will feel they live in a "parallel world" from non-users
- The emerging "AI economy" will evolve rapidly—faster than even crypto did—but touches more of regular economic reality
- Progress feels "ghostly" even to practitioners: no visible drones or robots, yet invisible digital transformations accelerate
- The funnel from casual user to power user narrows dramatically on curiosity, access, prompt formulation, and experimentation time
- Clark uses a higher-dimensional metaphor: AI exists partially beyond normal observable reality, visible only as glimpses passing through our four-dimensional space

---

### [Fifty Years of Progress](https://x.com/esrtweet/status/2004829013068444050)
*2026-02-11T20:00:00Z* | Tags: ai-progress, computing-history, singularity, personal-reflection

Eric S. Raymond (pioneering open-source developer, author of "The Cathedral and the Bazaar") reflects on 50 years of computing progress after using GPT-4.1 through Aider for AI-assisted coding.

**Key Points:**
- 1975 baseline: Programs run via punched cards on calculators; Unix/C hadn't left Bell Labs; DOS six years away; global computing capacity roughly equivalent to one modern smartphone
- Primitive tooling: Teletypes as production gear; no version control; no public forges; pixel-addressable color displays were science fiction
- Programming velocity: Code production was "slow and laborious"—monthly output was tiny by modern standards
- Present day: "Call spirits from the vasty deep"—conversing with AI to produce in a single day programs that would once have been "prohibitively complex to attempt"
- Personal milestone: Still coding after 50 years, adapting through every technology generation
- Prediction failure: Even as a science-fiction fan, would have predicted pocket devices with real-time world knowledge access multiple centuries away
- Conclusion: "The Singularity is upon us. Everything I've lived through and learned was just prologue."

---

### [Claude Code Creator: One Year of Progress](https://x.com/i/status/2004887829252317325)
*2026-02-11* | Tags: ai-progress, capability-trends, productivity-metrics, claude-code

Boris Cherny, creator of Claude Code, reflects on the tool's evolution and shares concrete productivity metrics demonstrating AI capability advancement.

**Key Points:**
- Claude Code began as a side project in September 2024
- One year ago: Claude struggled to generate bash commands without escaping issues; worked seconds to minutes at a time
- Current state: Claude consistently runs for minutes, hours, and days (using Stop hooks)
- 30-day metrics: 259 PRs, 497 commits, 40k lines added, 38k lines removed—all AI-generated
- "Code is no longer the bottleneck"—software engineering is fundamentally changing
- Describes the technology as "alien and magical"

---

### [Relative Competition and Work Hours](https://x.com/i/status/2003380683876503718)
*2026-02-12T13:28:06Z* | Tags: ai-progress, productivity, competition, labor, startups

Thread arguing that better AI tooling does not necessarily reduce hours worked, because work is often relative to competitors. The author points to AI startups with large AI tool budgets whose developers still work long hours to outcompete other AI-native teams.

**Key Points:**
- In AI startups, there is often no practical limit on spending for AI dev tooling, and teams use it heavily
- Even with strong AI tooling, competitive pressure can increase rather than decrease working hours
- When everyone adopts better tools, the bar rises: teams must deliver either higher quality or more output to be “best in the industry”
- Tool gains may be reinvested into iteration speed and scope rather than leisure time

---

### [Epistemic Trust](https://x.com/JaimeObregon)
*2026-02-13T12:00:00Z* | Tags: information-overload, epistemic-trust, ai-era, trust, attention-economy

Jaime Gómez-Obregón reflects on how the promised "information age" has become an era of information overload—permanent cognitive overwhelm that AI's exponential growth only accelerates. With attention as the new scarcest resource, the temptation to exaggerate and theatricalize truth grows. Impact replaces rigor; the emotional displaces the intellectual.

**Key Points:**
- The 1990s "information superhighway" optimism gave way to information overload and permanent cognitive overwhelm
- AI accelerates the information avalanche—too many messages, too many voices, too many shouted certainties
- Careful curation of information sources is essential: if you choose what goes into your stomach, why feed information-junk to your brain?
- In the AI era, we return to something deeply human: trusting not messages but people—those who have demonstrated rigor, intellectual honesty, and an authentic voice
- Peter Fonagy's concept of "epistemic trust": humans evaluate not just content but whether the source deserves to influence their understanding of reality
- Knowledge transmission requires connection and credibility, not just arguments
- Epistemic trust means deliberately choosing who influences how you understand reality—carefully, precisely because everything seems to say everything, all the time

---

### [Spotify says its best developers haven't written a line of code since December, thanks to AI](https://techcrunch.com/2026/02/12/spotify-says-its-best-developers-havent-written-a-line-of-code-since-december-thanks-to-ai/)
*2026-02-14T11:40:40Z* | Tags: ai-progress, capability-trends, enterprise-adoption, ai-coding, claude-code, spotify

TechCrunch reports comments from Spotify's Q4 earnings call describing a workflow where AI (including Claude Code) handles much of the direct code-writing and deployment steps, with engineers shifting toward prompting, review, and rapid shipping.

**Key Points:**
- Spotify co-CEO Gustav Söderström said the company's best developers "have not written a single line of code since December"
- Spotify described an internal AI system ("Honk") used to increase coding speed and product velocity
- Example workflow: from a phone via Slack, an engineer asks Claude to fix a bug or add an iOS feature, receives a new app build back in Slack, then merges to production before reaching the office
- Spotify credited the system with speeding up coding and deployment "tremendously" and framed this as "just the beginning"

---

### [xAI Operations and Western Competitiveness](https://x.com/i/status/2022491197667774573)
*2026-02-15T12:00:00Z* | Tags: ai-progress, geopolitics, competition, organizational-speed, talent

Twitter thread analyzing xAI's operational model as a response to global AI competition dynamics, arguing that organizational speed is the key competitive variable.

**Key Points:**
- xAI engineer described environment: "There isn't organizational overhead getting in your way, having to write docs. You just do stuff."
- Claimed statistic: China controls approximately 50% of the world's AI researchers
- Western companies characterized as burdened by compliance reviews, documentation mandates, approval hierarchies, risk assessments
- Talent advantages compound generationally—elite researchers train the next wave, and speed multiplies the compounding rate
- Western governance structures (ethics committees, responsible AI initiatives) framed as "valuable in peacetime" but competitive liabilities in an AI race
- Traditional Western advantages (capital markets, research institutions) become irrelevant if the output gap widens because "one side builds while the other holds meetings about building"
- Stark framing: "The choice isn't between chaos and order. It's between execution and extinction."

---
